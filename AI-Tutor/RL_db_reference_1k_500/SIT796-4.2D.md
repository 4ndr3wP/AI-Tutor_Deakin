## SIT796 Reinforcement Learning

Distinction Task 4.2: Dynamic programming , Monte-Carlo Methods

## Overview

During week 3, you have learnt about Dynamic Programming and MDPs, and in week 4, you learnt about on and off policy approaches.

- (a) You are given a robotic arm shown in the figure above, for which you need to design a controller to do some task (say, picking up objects from a table). Which of the two approaches -(i) Dynamic programming and (ii) Monte-Carlo methods would you consider for this? What are their possible benefits and drawbacks?
- (b) As you are working with the robot, one of the components fail, and the robot is unable to move one of its joints. The other joints still work normally. Which of the above two controllers is more likely to adapt to the faulty robot without any change to the controller code? Why?
- (c) Use the navigation environment from the week 2 workshop and find the optimal policy the using Monte-Carlo Control.
- (d) Implementing (b) in a 100x100 environment could be challenging. Why? What steps could be taken to make the implementation easier? (You do not actually have to implement it in the 190x100 environment. Just provide answers.)
- (e) Consider an environment with discount factor \gamma=o.g. Your associate is developing a new algorithm, and they report that after running several trajectories, the maximum and minimum possible instantaneous rewards achievable in this environment are 1 and -1., and that the most valuable state has a value V(s) = 23.2. You immediately tell them to recheck their algorithm. Why?

Hint: Start from the fact that V(s) = E[7m tyr ty? +--Â°]

## Submission Details

For this task, please submit screenshots of your implemented code. For the written components of this task, please provide clear explanations.

## Constraints

Keep your answers short and concise. Try not to exceed 600-800 words in total for the written components.