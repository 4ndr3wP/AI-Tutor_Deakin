SIT796 Reinforcement Learning

Function Approximation

Presented by: Thommen George Karimpanal School of Information Technology

<!-- image -->

## Approach so far

## Tabular approach:

- ¢ Single entry in a table of values
- ¢ States and actions have discrete representations
- * Working set of algorithms
- * Convergence proofs

| State   | S,   | ay | Q(s,a,)   | a2 | Q(s;az)   | a3 | Q(s; a3)   |
|---------|------|----------------|----------------|-----------------|
|         | Sy   | | Q(Sp.a,)     | |Ql(S2a2)      | | Q(s, a3)      |
| Action  | S3   | | Q(s3.a;)     | |Q(S3a2)       | | Q(s3 a3)      |
|         | Sa   | Q(S4,a,)       | Q(S4,a2)       | Q(S,4,a3)       |

<!-- image -->

<!-- image -->

## Limitations of Tabular Approach

<!-- image -->

<!-- image -->

## Function Approximation

## Let's first consider state value prediction v,, with function approximation.

- ¢ Instead of representing each state in a table of values, we represent it in a parameterised functional form using a weight vector w € R@
- ¢ Hence, we write the value for the approximate value of state s as given the weight vector w: B(s,w) = v,(s)
- — The number of dimensions of the vector is strictly much less the number of raw states d &lt; |S].
- — Hence changing the value of one weight changes the value of many weights
- — Problem now is to find a w* that best approximates v,,(s) (or Q,-(s, a))

## Examples of what @ might be

Could be a linear function of features in the state where w is the weight of each feature

Could be non-linear function of features in the state computed over an Artificial Neural Network

Could be function computed by a decision tree where wis all the numbers defining the split points of the tree

<!-- image -->

<!-- image -->

<!-- image -->

## Linear Methods

A special case of function approximation commonly used Linear function approximation.

- ¢ That is our approximation function 0(-, w) is linear function over the weight vector w.
- * Where each state s is represented with a vector x(s) = [x,(s), x2(s), +++, Xq(s)]", where |x(s)| = |w|
- * Now we can represent our state-value function using the inner product of w and x(s)

<!-- formula-not-decoded -->

- * Here x(s) is referred to as the feature vector for the state s.

It worth noting that the new notation can still represent the tabular approaches seen so far.

For instance, represent the state as a vector of the table entries. The feature representing the location of the agent is set to 1 and all others to 0.

<!-- image -->

x(s) = [0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0]

w = [0.0,0.1,0.2,0.4,0.1,0.3,0.4,0.6,0.2,0.4,0.6,0.8,0.4,0.6,0.8,1.0]

<!-- formula-not-decoded -->

<!-- image -->

## State Aggregation

Probably the simplest approach for function approximation is State Aggregation.

- ¢ The aim is simply to group similar states together and treat them as a single state
- ¢ The grouped states are represented with a single value within the weight vector

## For example

Image we have a grid world with 10,000 grids in two dimensions = 100 million discrete states.

Recall the more states then the more values to learn and less often we get to visit them relative to all the states

Now if we know an action may move us between one step up to 100 grids during each time step then we may group 50 states of each dimension as a single state.

This will reduce our states to 200X200 = 40,000 states

<!-- image -->

## This is 1 state now

<!-- image -->

Faster learning but each group of 50 states will each be assigned a single common value.

SIT796 Reinforcement Learning

Stochastic Gradient MC

Presented by: Thommen George Karimpanal School of Information Technology

<!-- image -->

## Stochastic Gradient MC

In Gradient Descent our aim is to reduce the error of all our examples.

- However, this is not useful in RL because we do not have all the examples when learning online through interaction.

Stochastic Gradient Descent (SGD) allows us to use a single example by adjusting our weight vector w in the direction of the estimated error, governed by a small factor a.

- To do this we must be able to identify the direction of the error, hence our function 0(s, w) must be differentiable.
- — We find the slope of a function by finding its partial derivative in this case with respect to w

<!-- formula-not-decoded -->

## Input:

- * Now when calculating an update we move the weight vector towards a : . . . smaller error by moving a small amount in the direction of the error.

<!-- formula-not-decoded -->

- ¢ Inthe linear case this reduces to:

<!-- formula-not-decoded -->

E.g. the gradient for the example on the previous slide is

VO(St, Ww) = [0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0]

the policy 1 to be evaluate 3 3 P a. d A differentiable function 0: 5xR° &gt;

Algorithm Parameter:

. Step size a € (0,1]

Initialise:

Loop forever (for each episode):

a weR arbitrarily eg.w = 0

Generate an episode based on Tt: So, Ao, Ry, $4, A1, Ro, .--, Sp-1, AT-1, Rr Loop for each step of the episode t = 0,1,...,T — 1:

w= w+ alG; — 0(S;, w)]VO(S;, w)

R

<!-- image -->

## Semi-Gradient TD(0)

<!-- image -->

When using applying these ideas to bootstrapping methods (e.g. TD) then we are basing updates on an estimate rather than the true value.

Semi-gradient methods need to replace the true target with an estimate of the target

<!-- formula-not-decoded -->

Input: the policy mt to be evaluate A differentiable function 0: SxR? &gt; R Algorithm Parameter: Step size a € (0,1] Initialise: w € R@ arbitrarily e.g. w = 0 Loop forever (for each episode): Initialize S Loop for each step of the episode until S € S(Terminal) Choose A~7(|S) Take action A, observe R, S' w=weal[R +yi0(S', w) — 0(S, w)]VO(S, w) S&lt;S'

Why semi-gradient?

When we update w, we take into account the change in estimate, but don't take into account the change in the target (which also depends on w)

So the gradient computed only represents part of the true gradient

## Semi-Gradient TD(A)

Recall we were able to use eligibility traces to define an algorithm that sat between MC and TD(0).

- We obviously also wish to have this ability when using function approximation.
- application to continuing problems. This will allow us to update the weight vector each iteration, balance computation throughout instead of just at the end, and allow its

Instead of defining a trace for each state, we define it as a vector z, € R%,s.t.|z| = |w|

Z\_4 = 0

<!-- formula-not-decoded -->

0 &lt;t&lt; T

Now we can update the weight vector proportionally to the trace vector

<!-- formula-not-decoded -->

Where the TD-error is calculated the same as TD (0) Ot = Rea + YO(Sta1, We) — (Sp, We)

Instead of having to iterate through each e(s) as we did in the tabular case, we simply update the trace vector in a single operation

Input:

The policy mt to be evaluate A differentiable function 0:SxR¢ &gt; R

Algorithm Parameter:

Step size a € (0,1]

Trace decay rate A € [0,1]

Initialise:

w € R@ arbitrarily e.g. w = 0

Loop forever (for each episode):

Initialize S

Reset z = 0

Loop for each step of the episode until S € S(Terminal)

; A, R, w)

Choose A~zr(- |S)

Take action observe S’

Z&lt; yAz + Vi(S,

6&lt;—R+y0(S',w) — 0(S,w)

w=w+e a6z

SoS’

<!-- image -->

SIT796 Reinforcement Learning

Function approximation with Control

Presented by: Thommen George Karimpanal School of Information Technology

<!-- image -->

## Semi-Gradient Control

Now that we have a method of prediction, we can investigate a method of control.

- ¢ Now we want to approximate the action-value function G = q,.
- ¢ That is, we will represent the parameterized functional form using a weight vector w.

Hence, One-step Sarsa with function approximation is defined with the update rule

<!-- formula-not-decoded -->

Control, however, is not just mapping the state to our vector w but also our actions.

For discrete actions this is perfectly fine — use a matrix where each column represents one of the possible actions.

Can also use a function approximation, such as action aggregation approach on the actions

However, every action increases the dimensionality of our value function

Neither of these are always suitable — Sometimes we want to have precise actions

E.g. The angle we turn the steering wheel on a car must be very precise to avoid an accident

This type of continuous action control is very much still an open question.

For now, we will assume there is a manageable discrete set of actions to select from

<!-- image -->

## Semi-Gradient Sarsa(0)

w=wtalR+yG(S, A, w)]Vq(S, A, w), special case for terminal state can't include future state

Input: A differentiable state-action value function 9: Sx AxR?¢ &gt; R A policy zz if predicting or q,, if estimating (e.g. using e — greedy) Algorithm Parameter: Step size a € (0,1] Initialise: w € R@ arbitrarily e.g. w = 0 Loop forever (for each episode): S,A &lt; Initial state and action of episode (e.g. using € — greedy) Loop for each step of the episode until S € S(Terminal) . Take action A, observe R, S" If S' © Sterminal then: else: Choose A' as a function of G(S',-, w) (e.g. using € — greedy) w=wtal[R + yG(S', A', w) — G(S,4, w)]VG(S, A, w)

S&lt;S'

A&lt;A'

<!-- image -->

## Sarsa(A) with Linear Function Approximation

Sarsa can also be extended to use eligibility traces similar to TD (A) — Note: This assumes binary features

```
Input: A policy z: if predicting or q,, if estimating (e.g. using e — greedy) A feature function F(s, a) returning the set of active feature indices for s,a Algorithm Parameter: Step size a € (0,1] Trace decay rate A € [0,1] Initialise: w € R¢ arbitrarily e.g. w = 0 Loop for each episode: Initialise S Choose A~z(|S) initial action of episode (e.g. using € — greedy) Reset z = 0 Loop for each step of the episode until S' is a terminal state: Take action A, observe R, S' 6-RDier(s,a) Wi for alli € F(S, A): zZ<—2Z+10rz« 1 depending on using accumulating or replacing traces If S' € Sterminal then: wewe aoz else: Choose A'~z:(|S') action (e.g. using € — greedy) 66+ Y Lier (s',A") Wi wewe a6z Z<—yAz SS'; ayy
```

<!-- image -->

## Semi-Gradient Off-Policy Control

There is a function approximation version of Watkins's Q-Learning that has been very popular.

- ¢ Below is a binary features version of the approach which aligns with the Sarsa implementation.

<!-- image -->

<!-- image -->

Note: for most of this unit we . are using Sutton and Barto (2018) — This algorithm, however is based on Sutton and Barto (1998) version as they no : a longer include it in the new version of the book. The reasons discussed on the following slides. However, | include as it is still in common use

## The Deadly Triad

<!-- image -->

Watkins' Q-Learning with function approximation however has been found to be unstable and does not always converge.

"the danger of instability and divergence arises whenever we combine all of the following three elements, making up what we call the deadly triad."

- ¢ Function approximation
- ¢ Bootstrapping
- ¢ Off-Policy training

Can we give up one of them? Presence of any 2 appears to be manageable (leads to stable learning)

Function approximation — Can not be replaced in large state spaces without introducing the Curse-of-Dimensionality

Bootstrapping — Is possible but at the cost of significantly more computation and loss of efficiency with increased memory costs

Off-Policy training — we can just use on-policy methods instead and often that is good. However, there are many use cases where we want to learn multiple policies simultaneously in parallel.

## Stable Off-policy Methods

There are a number of approaches recently that provide stable off-poly methods with function approximation.

- For instance, one approach might be to select off-policy behaviours that are close to the target policy can be effective

## Gradient-TD methods.

- ¢ — Aim to minimize the Projected Bellman Error instead of reducing the TD-error
- ¢ They achieve this but effectively double the computational complexity

## Emphatic-TD methods.

- These methods rewrite the state transitions using importance sampling s.t. they are appropriate for learning the target policy
- — It does this while using the behaviour policy distributions

## These methods however are not explored in these classes.

- ¢ They can be explored for your major research task

<!-- image -->

SIT796 Reinforcement Learning

Feature Construction

Presented by: Thommen George Karimpanal School of Information Technology

<!-- image -->

## Function Construction

For function approximation, we need to convert the state-value function to feature vectors.

This can be done using any of the following:

- Polynomial features
- Fourier basis
- Coarse coding
- Tile coding
- Sparse coding
- Dictionary Learning

Consider the state s represented to be represented by a vector x(s) = [x,(s), X2(s), +++, xXq(s)]", where [x(s)| = |w| We can represent our state-value function using the inner product of w and x(s)

<!-- formula-not-decoded -->

<!-- image -->

## Polynomial Features

<!-- image -->

In a polynomial feature, x(s) is a polynomial basis, that is, for a set of states s = {51, Sz, 53, ..., Sx}, the polynomial features can be written as

<!-- formula-not-decoded -->

where cj ; is a non-negative integer such that c;; = {0,1, 2,...,n}

Thus, we have

<!-- formula-not-decoded -->

and x,(s) is a n-order polynomial basis in a k-dimensional space spanned by (n + 1)* different features

## Polynomial Feature Example

There are several kinds of polynomial bases, such as:

- e Lagrange
- ¢ Newton
- * Orthogonal polynomials
- * Chebyshev

The Newton basis functions are;

<!-- formula-not-decoded -->

<!-- image -->

<!-- image -->

Which gives, in the third order case, the following

<!-- formula-not-decoded -->

SIT796 Reinforcement Learning

Fourier Basis

Presented by: Thommen George Karimpanal School of Information Technology

<!-- image -->

## Fourier Transform

- ¢ Polynomials are not the best -unstable and not very physically meaningful.
- * Easier to talk about "signals" in terms of its "frequencies" (how fast/often signals change, etc).
- ¢ Any periodic function can be rewritten as a weighted sum of Sines and Cosines of different frequencies (Jean Baptiste Joseph Fourier, 1807).

The aim is then to understand the frequency @ of our signal.

So, let's reparametrize the signal by @ instead of x:

<!-- image -->

For every w from 0 to inf, F(@) holds the amplitude A and phase ¢ of the corresponding sine

<!-- formula-not-decoded -->

<!-- image -->

<!-- image -->

## Fourier Transform

Represent the signal as an infinite weighted sum of an infinite number of sinusoids

<!-- formula-not-decoded -->

Note:

<!-- formula-not-decoded -->

Arbitrary function

©

——}&gt;

Single Analytic Expression

Spatial Domain (x)

——&gt;

Frequency Domain (u)

(Frequency Spectrum F(u))

Inverse Fourier Transform (IFT)

<!-- formula-not-decoded -->

<!-- image -->

## Fourier Transform

This means that F can encode both using the imaginary numbers.

Consider

<!-- formula-not-decoded -->

The amplitude is given by

<!-- formula-not-decoded -->

and the phase is

<!-- formula-not-decoded -->

<!-- image -->

Square Wave with 50% duty cycle Fourier transform

<!-- image -->

## Fourier Features

In a Fourier basis, the set of states s = [S1, Sz, S3,..., sl? can be expressed using a cosine in the following way x;(s) = cos(ms'c*)

<!-- formula-not-decoded -->

<!-- image -->

Thus, we have:

<!-- formula-not-decoded -->

<!-- image -->

SIT796 Reinforcement Learning

Coding Methods

Presented by: Thommen George Karimpanal School of Information Technology

<!-- image -->

## Coarse Coding

- * Coarse coding is used for continuous spaces, but can also be used for state-spaces that are binary or to further encode other features.
- ¢ ach ball or sphere is usually called a receptive field.
- ¢ Features with large receptive fields give broad generalization, but might yield very coarse approximations
- ¢ The trade-off is often a question to be solved for their implementation:
- -Too coarse may not be discriminative enough
- -Too large and the complexity may increase too much

<!-- image -->

<!-- image -->

<!-- image -->

Narrow generalization

<!-- image -->

<!-- image -->

Broad generalization

Asymmetric generalization

The type of receptive field affects the nature of generalisation

<!-- image -->

## Coarse Coding

approx-

<!-- image -->

<!-- image -->

## Tile Coding

- Makes use of tiles and tilings
- Tiles are elements of tilings
- If only 1 tiling is used — state aggregation
- Many tilings are used to obtain the ability to represent the state space as finely or coarsely as required (Generalisation)
- Uses binary features
- Tilings are offset from each other uniformly in each dimension

<!-- image -->

<!-- image -->

## Tile Coding

- ¢ Tilings are offset from each other : : : : uniformly in each dimension
- . ¢ Diagonal elements become too prominent
- * To fix this, the offset can be done unsymmetrically

<!-- image -->

<!-- image -->

## Readings

## This lecture focused on exploring how we can use function approximation in RL.

- ¢ Future topics will expand on this topic by looking at Deep RL.
- e Ensure you understand what was discussed here before doing the following topics

For more detailed information see Sutton and Barto (2018) Reinforcement Learning: An Introduction. Several illustrations here were borrowed from the book.

- ¢ Chapter 9.5: Feature Construction for Linear Methods
- . . ° http://incompleteideas.net/book/RLbook2020.pdf

## Other readings:

- ¢ Isabelle Guyon, Steve Gunn, Masoud Nikravesh, and Lofti Zadeh (Eds.), "Feature Extraction: Foundations and Applications", Springer, 2006.
- ° http://www.causality.inf.ethz.ch/ciml/FeatureExtractionManuscript.pdf
- ° https://sites.fas.harvard.edu/~cs278/papers/ksvd.pdf

<!-- image -->

<!-- image -->