<!-- image -->

## SIT796 Reinforcement Learning

Credit Task 3.2: Exploration/Exploitation Dilemma and Choice of DP/RL

## Overview

During week 2, you were introduced to multiarmed bandits and action selection mechanisms. For this task, you may need to refer to lecture slides from week 2 and week 3, as well as the week 2 workshop code.

## Submission Details

- 1. (a) Using the reinforcement learning code and MiniGrid-Empty-6x6-vO environment provided in the workshop session in Week 2, implement a linearly decaying epsilon strategy (note that the original workshop code contained an exponentially decaying epsilon). That is, start with epsilon=1 and reduce it by 0.005 after each episode. Ensure that the epsilon value is at least 0.01 in every episode. Maintain all other hyperparameters unchanged.
- (b) Now run the code with a fixed epsilon value of i) 0.1 and ii) 0.9.
- (c) In the same plot, using different colours, plot the reward curves for the linearly decaying epsilon strategy and compare it with the strategy of fixed epsilon values of 0.1 and 0.9. Explain what you observe from the three reward curves in the plot.
- 2. Consider the automated cleaning robot from task 2.1P. You are asked to develop a controller for this robot based on dynamic programming. What are the aspects you will consider before deciding whether dynamic programming is the right choice of algorithm for it?

## Constraints

Provide screenshots of the code and the plot. Keep the explanations short and to the point. Try not to exceed 500-600 words in total.