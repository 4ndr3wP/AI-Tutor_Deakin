SIT796 Reinforcement Learning

Machine and Reinforcement Learning in History

Presented by: Dr. Thommen George Karimpanal School of Information Technology

<!-- image -->

<!-- image -->

Agent

<!-- image -->

Richard Bellman (1950s)

<!-- image -->

—

<!-- image -->

<!-- image -->

Richard Sutton and Andrew Barto (since 1980's)

<!-- image -->

## Artificial Intelligence Throughout History

## Jacquard loom of early 1800s

- e Translated card patterns into cloth designs

## Charles Babbage's analytical engine (1830s &amp; 40s)

- e Programs were cards with data and operations

## Ada Lovelace — first programmer

"The engine can arrange and combine its numerical quantities exactly as if they were letters or any other general symbols; And in fact might bring out its results in algebraic notation, were provision made.

<!-- image -->

<!-- image -->

<!-- image -->

## The First Computers

## 1946John Von Neumann

led a team that built computers with stored programs and a central processor

ENIAC, however, was also programmed with patch cords.

<!-- image -->

Von Neuman with ENIAC

<!-- image -->

## Artificial Intelligence Throughout History

## e 1950 -Alan Turing

- Publishes "Computing Machinery and Intelligence
- Proposes "the imitation game" which will later become known as the "Turing Test."

## e 1951Marvin Minsky and Dean Edmunds

- Build SNARC (Stochastic Neural Analog Reinforcement Calculator)
- This is the first artificial neural network, using 3000 vacuum tubes to simulate a network of 40 neurons.

<!-- image -->

## Artificial Intelligence Throughout History

- August 31, 1955 -John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon
- ¢ The term "artificial intelligence" is coined
- They propose a "2 month, 10 man study of artificial intelligence"
- Submitted to the workshop at Dartmouth College
- ° Considered as the official birthdate of the new field

## December 1955 -Herbert Simon and Allen Newell

- They develop the Logic Theorist, the first artificial intelligence program
- "The Theorist" proved 38 of the first 52 theorems in Whitehead and Russell's Principia Mathematica.

<!-- image -->

## Artificial Intelligence Throughout History: Decision Making

<!-- image -->

<!-- image -->

## Artificial Intelligence Throughout History

## ¢ McCulloch &amp; Pitts

- e Pitts: self-taught genius, left home at 15
- ¢ McCulloch was impressed by Pitts, homeless at the time
- ¢ Collaborated to write the seminal paper: "A Logical Calculus of Ideas Immanent in Nervous Activity" -First mathematical model of a neural network
- e Resea rch findings that the brain Was not solely responsible for image processing
- ¢ Burned his unpublished work on 3D neural networks

Warren McCulloch (L) and Walter Pitts (R)

<!-- image -->

<!-- image -->

neta: //donaldclarkplanb. blogspot.com/2021/11/mecullochpitts-neural-nets.htm|

## Artificial Intelligence Throughout History

## Frank Rosenblatt's Perceptron (1957)

- e An electronic device based on a single neuron, able to learn to classify 20x20 images.
- ¢ Marvin Minsky showed in his 1969 book Perceptrons that perceptrons were fundamentally limited.
- e A few years later, this problem was addressed by the development of multi-layer perceptrons.

Frank Rosenblatt uoit.ca/courses /ist/notebooks/nn-

<!-- image -->

<!-- image -->

Source: http://csgrad.science. history.html

Marvin Minsky

<!-- image -->

## Artificial Intelligence Throughout H istory

## Al Winter (1969-2006)

- ¢ Minsky &amp; Papert's findings in Perceptrons (1969) significantly impacted the field
- ¢ Funding cuts, fewer and fewer researchers working on Al
- ¢ The community moved to more grounded approaches like support vector machines (SVM)
- ¢ Meanwhile, a few researchers still persevered
- e And PC gaming led to more and more powerful GPUs

<!-- image -->

<!-- image -->

<!-- image -->

Geoff Hinton (Deep learning), Richard Sutton (Reinforcement learning) and Jurgen Schmidhuber (LSTMs)

<!-- image -->

## Artificial Intelligence Throughout History

## Al Summer (2006-now)

- ¢ Deep Belief Nets (2006) by Hinton
- ¢ AlexNet (2012) showed significant performance improvements in the ImageNet challenge (database of over 20000 object categories for visual object recognition)
- ¢ Deepmind: super-human Atari playing capabilities (2015)
- ¢ Transformers, GPT (2021), Sora (2024), DeepSeek (2025)
- e Avery hot Al summer!

<!-- image -->

## RL: Relevance

Keywords based on ICLR2022 data

<!-- image -->

<!-- image -->

## RL: popularity

<!-- image -->

Google trends: Reinforcement Learning

<!-- image -->

SIT796 Reinforcement Learning

Machine and Reinforcement Learning

Presented by: Dr. Thommen George Karimpanal School of Information Technology

<!-- image -->

## Machine and Reinforcement Learning

## Artificial Intelligence

Systems designed to behave in such a way that they appear intelligent.

- ¢ Reasoning (logical Inference)
- * Learning (Machine Learning)

## Machine Learning

## Supervised Learning

Aims to generalize 'ining thet tagged wn the correct answer to solve previously unseen data.

<!-- image -->

~~

## Reinforcement Learning (RL)

Systems that learn and adapt over time based on TReneeoleietennal nani eition

## Semi-Supervised Learning

Aims are similar to Supervised Learning but uses a comb--oe : : ination of tagged and untagged data. Used in areas like . . . anomaly detection where certain tags may be unavailable.

hee hs plane nee Sah ecision-making tasks through trial-and-error rather than through tagged examples.

## Unsupervised Learning

. . . Aims to find pattern using untagged data and can be . . . used to identify trends or unknown groupings.

## Machine and Reinforcement Learning

<!-- image -->

<!-- image -->

## Examples of Reinforcement Learning (1) -TD Gammon

<!-- image -->

## TD Gammon (1992).

- ¢ Finished in the top 10 players of the world
- ¢ While not called it at the time — TD Gammon was an early success of Deep RL.
- — Used a multilayer neural network
- — Used Temporal difference learning
- ¢ Played unique strategies that expert ended up adopting
- — Eg witha roll of 2-1, 4-1 or 5-1 expert typically used a technique called "slotting" move from point 6 to point 5.
- — Expert, Bill Robertie did a rollout analysis of TD-Gammon's "split approach of moving from point 23 to 24, and found it was more effective. "
- — This is now the standard opening move

<!-- image -->

## Agent57

- ¢ Could out-perform human benchmarks on all AtariS7 games.

## Alpha Go (2015).

- ¢ Used Monte Carlo Tree search (Coulom 2006) and learnt from both self and human games.
- ° Defeated Lee Sedol 4 games to 1 (2016). First program to beat a 9-dan Go champion on a 19x19 board without a handicap.
- ¢ Televised internationally, and a documentary movie released.

<!-- image -->

<!-- image -->

<!-- image -->

Sutton and Barto (2018)

https://images.tech hive.com/images/article/2016/03/go-game-screen-shot-2016-03-08-at-8.17.43-pm-pst-100649230-large.jpg

Winands M.H.M. (2017) Monte-Carlo Tree Search in Board Games. In: Nakatsu R., Rauterberg M., Ciancarini P. (eds) Handbook of Digital Games and Entertainment Technologies . Springer, Singapore. https://doio1g/10.1007/978-981-4560-50-4\_27

<!-- image -->

https://www.youtube.com/watch ?v=WxXuKk6gekU1Y

<!-- image -->

## AlphaZero (2018).

- * Can now play different games eg chess and shogi

## MuZero (2019).

- Can learn any game without being told anything about the rules
- — Learnt Go better than Alpha Zero. Better on Atari

## Chat GPT (2 023)

Uses reinforcement learning to decide the more human-like response (RLHF)

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

## Examples of Reinforcement Learning (4)

## Many other application areas

- Robotic control, autonomous cars, drones, autonomous underwater vehicles, ...
- Energy plant , manufacturing, warehouse operations, logistics, ...
- Trading and finance, healthcare, recommendation, marketing....

<!-- image -->

<!-- image -->

SIT796 Reinforcement Learning

Reinforcement Learning Overview

Presented by: Dr. Thommen George Karimpanal School of Information Technology

<!-- image -->

## An Example:

The agent (robot) knows nothing about the environment

Possible actions: left, right, up, down

<!-- image -->

What should it do?

Try out different actions and see what happens

Trial and error!

<!-- image -->

<!-- image -->

## Motivation — Learning from Experience

<!-- image -->

<!-- image -->

We learn a number of skills by trial and error

But how and what do we actually learn?

<!-- image -->

<!-- image -->

## Motivation — Learning from Experience

## Two key challenges

- ¢ How to act in a way that is beneficial in the long term and not just short term?
- ¢ Temporal credit assignment: which of the previous actions were responsible for an agent's good/bad performance? Hard problem!

<!-- image -->

## Reinforcement Learning Involves:

- ¢ Optimization: Find an optimal way of making decisions
- ¢ Generalization: How well do these decisions apply to similar situations
- e Exploration: Use past experience to execute optimal actions, but also ensure that the agent is exposed to new experiences
- ¢ Delayed rewards: Consequences of current decisions may be experienced only in the future

<!-- image -->

<!-- image -->

SIT796 Reinforcement Learning

The Problem

Presented by: Dr. Thommen George Karimpanal School of Information Technology

<!-- image -->

## The Problem

Formally, the RL problem is formulated as a Markov Decision Process (MDP)

- * AnMODP is atuple M={S, A,J,y, R}
- — S§ -Theset of possible states
- — ATheset of actions the agent can take
- -— J -transition probabilities
- — y -The discount rate or the discount factor.
- -RAreward distribution function conditioned.

R=0

<!-- image -->

Rewa rd fu nction

Deterministic transition

<!-- image -->

Probabil istic transition

<!-- image -->

<!-- image -->

## The Environment

<!-- image -->

An agent is the learner and decision-maker — the environment is everything outside the agent that the agent does not directly control

- The boundary between agent and the environment may not be the physical boundary.
- For example, a robot's sensors, motors and actuators, while physically part of the robot, are often treated as part of the environment.
- This is the same as separating the human mind (agent) from the eyes, ears, skin, muscles and bones (environment)
- A simple rule is that anything that cannot be changed arbitrarily by the agent is part of its environment

<!-- image -->

<!-- image -->

## The state is the agent''s internal representation

- ¢ May only be a small portion of the environment.

## For example:

- e Agents generally operate in discrete time and hence perceive the environment as snap shots — like a film.
- * Location of chess pieces / number of pieces it attacks / places the piece can move.

The choice of state representation can significantly affect the agent's ability to learn a solution.

<!-- image -->

Markov Property: All of the agent's history is represented by the state

<!-- image -->

These are things the agent can do in the environment

Can be discrete or continuous

<!-- image -->

<!-- image -->

## Reward Function

<!-- image -->

The Reward Function defines the Goal of the Problem being learnt.

The reward function determines how much and when/where reward should be given.

- ¢ Note: the reward function is external to the agent.
- ¢ Therefore, the agent cannot alter it

Improperly designed reward functions can lead to undesirable behaviours.

<!-- image -->

Eg: Rapple=1 (terminal), R;,.=-1 (terminal), Rgeray4=0-1

Agent can collect an accumulate infinite rewards by avoiding apple/fire states!

<!-- image -->

## Value Function

The Value Function is an estimate of the total reward that an agent can expect to accumulate in the future in a given state.

Many ways to store/record a value function:

<!-- image -->

- ¢ State-value function V/s): indicates how valuable it is to be in state s
- ¢ Action-value function (or Q-value) Q(s,a): indicates how valuable it is to be in state s and take action a

RL aims to estimate the value/action-value function.

<!-- image -->

<!-- image -->

## Discount Factor

$100 now or $100 after 1 year?

Of course, now!

We value things in the future less

<!-- image -->

No discounting:

—

V(s)=0+0+0+1=1

With discounting (discount factor=0.9):

V(s)=0+0.9*0+(0.9)2"0+(0.9)3"1= 0.729

<!-- image -->

## Agent's Policy

## The value function provides a means to map an agent's states to the actions

- ¢ This mapping is called the agent's policy, 7.
- * Where 7;(s, a) is the probability of selecting action a at state s at time t.

You can think of the policy as being the way the agent has chosen to behave for a given state

- ° — It is this mapping (policy) that RL methods are attempting to learn.
- * Once learnt these probabilities should be the same as the optimal probability for each state, denoted 77; (s, a)

RL methods update their policy — attempting to maximise the total amount of reward over the long run.

After the value function is learnt, the best (optimal) action is simply the one that corresponds to the highest value.

But should the agent simply choose the action with the highest value all the time?

<!-- image -->

<!-- image -->

SIT796 Reinforcement Learning

Action Selection

Presented by: Dr. Thommen George Karimpanal School of Information Technology

<!-- image -->

## Action Selection (Exploration vs Exploitation)

A fundamental issue in Reinforcement Learning is the trade-off between:

- ¢ Exploration: Search to see if there are other, potentially better, ways to do your task
- * Exploitation: Keep doing what you have already learnt is best
- — Select the action with the highest value function

We will discuss a number of ways to address this dilemma: Multi-armed Bandits greedy, €-greedy, Upper Confidence Bounds, and Soft-max Optimistic Initialisation

<!-- image -->

<!-- image -->

<!-- image -->

## RT aya (Oe mM CCM gerd (ey))

<!-- image -->

Learning is the process of updating/changing/improving our understanding of the what is the best policy

- ¢ Tells the system the correct answer regardless of the answer actually given by the system.

## One approach is to use the Value Function.

- ¢ Recall: The Value Function is an estimate of our future expected reward.
- ° So if after executing an action we find we received more/less reward that we expected then we should update our value function

Therefore, if our value for a state, s, at time, t, was V(s;), then we can update it using the Bellman equation.

<!-- formula-not-decoded -->

Where a is set to a value between 0 &lt; @ &lt; 1. This is called the step-size parameter (or learning rate) a is sometimes set high initially and reduced overtime.

## Tasks

## A task in RL is an MDP {S,A,R,T}

- e¢ Represents one instance (epoch) of the RL problem

## Examples

- * Game of Chess
- ¢ Stand up for as long as possible
- ¢ Minimise customer wait times
- e¢ Extinguish a fire

## Two primary types of tasks:

Episodic tasks — is one that has a defined terminal condition when the task is completed. Eg: Chess, reach destination, solve problem

Continuing Tasks — tasks with no terminal condition. Eg: moving in a circular path, learning mathematics

<!-- image -->

<!-- image -->

<!-- image -->

## Prediction vs Control

## Reinforcement Learning is used to solve two classes of problem

## The Prediction Problem

- ¢ Aims to learn what value a particular state has V,(s) or the value of an action when taken from a state V;(s, a) usually given some example policy.
- ¢ The values learnt are not used to decide on what the agent will do — just to predict what the outcome will be for different policies
- ¢ For instance, predicting global warming given different potential policies.

## The Control Problem

- ¢ Aims to learn a control policy that identifies the best action to take given a particular state 7;,(s, a).
- e Used in decision-making tasks
- e Such methods need to continually update and improve the policy based on past experiences.
- ¢ For instance, controlling a robot for picking up rubbish.

<!-- image -->

<!-- image -->

<!-- image -->

## A Simple Maze Example (1)

<!-- image -->

Episodic task. Using a policy provided the aim is to learn a prediction of the expected value for each state.

Reward:

-1 per time-step

Actions:

up, down, left, right

State:

agent’s location

Optimal Policy m*(s, a) represented for each state s with a red arrow.

<!-- image -->

<!-- image -->

## A Simple Maze Example (2)

<!-- image -->

Reward Function: defines the amount of immediate reward.

Value Function: Stores a number representing the value (expected sum of rewards under a policy) for each state, v,,(s)

<!-- image -->

cu

<!-- image -->

## Readings

This was a quick overview of Reinforcement Learning.

- ¢ Intention was to introduce the main terminology and the RL learning process.
- * Following topics will delve deeper into the topics discussed here.
- ¢ Ensure you understand what was discussed here before doing the following topics

For more detailed information see Sutton and Barto (2018) Reinforcement Learning: An Introduction

- * Chapter 1: Introduction
- ° http://incompleteideas.net/book/RLbook2020.pdf

<!-- image -->

<!-- image -->