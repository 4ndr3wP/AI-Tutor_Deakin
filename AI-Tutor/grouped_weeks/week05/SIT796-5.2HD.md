## SIT796 Reinforcement Learning

## High Distinction Task 5.2: TD learning

## Overview

<!-- image -->

- (a) The above figure shows a room in which a robot starts moving from the shown position. The green lines show the random route taken by the robot while it was exploring the room. The robot manages to explore all areas except the area shaded in gray. We have recorded the trajectory (the states and actions â€” which are assumed to be discrete) taken by the robot along the green lines.
- 1. Using the collected data, we want to learn a policy to navigate to the goal location ga (each goal is marked by a pink square region). What algorithm will you use, and how would you go about learning a policy to go to g1? Describe the steps you would take.
- 2. Now that we have learned how to go to g1, we want to go to gz. Is it really possible to learn another policy (going to g2) with the same data? Again, describe the algorithm you would use and the steps you would take to learn this policy.
- 3. Nowthe user wants the robot to also learn a policy to go to g3. Again, is this possible to learn using the same data? Describe how you would learn to go to g3.
- (b) In the week 5 workshop, you saw that SARSA learned safer policies compared to Q-learning. Suggest 3 modifications (come up with your own ideas along with the rationale) to the Qlearning algorithm to make it generate safer policies. Implement one of these ideas in the cliff world environment. Compare its performance to SARSA. Is it doing better or worse? Why?
- (c) In the cliff world environment, run both Q-learning and SARSA by setting epsilon=1 (always choose random actions). Plot the learning curve (sum of rewards vs episodes). As you just took random actions in the environment, the reward curves just look random, and it probably does not look like the agent is learning. Now, just as you computed and plotted the sum of rewards vs episodes, plot the sum of absolute values of the TD errors in each episode vs episodes. Run both algorithms for the same setting (epsilon=1). Compare these absolute TD error plots for Q-learning and SARSA -explain what you observe.

## Submission Details

Some of these questions have no single correct answer. There may be many possible answers. Include your code in the submissions. Answer as briefly and accurately as possible.

<!-- image -->